{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "mount_file_id": "1kFkB9bIffTcVUR0DmYC3DgQSJYzTG1aC",
      "authorship_tag": "ABX9TyNYPW4jjfDvR6tisEoOdLSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayorovKonstantin/Test/blob/main/PetProject_of_HousePrice_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/10V00AzU0Ns0s13Ps2o3G33vlDFGOfeKD/view?usp=sharing"
      ],
      "metadata": {
        "id": "oqYPZ12vel_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data"
      ],
      "metadata": {
        "id": "R8LVprsfd5xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 10V00AzU0Ns0s13Ps2o3G33vlDFGOfeKD"
      ],
      "metadata": {
        "id": "ST4yS7lad_na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiYpsk-7du6n"
      },
      "outputs": [],
      "source": [
        "! unzip /content/house-prices-advanced-regression-techniques.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "NMbkzNF7fx7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA\n"
      ],
      "metadata": {
        "id": "6_68xbCv74z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "qFDVhzDN78Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "YkQuGVnlQBMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ID = train['Id']\n",
        "test_ID = test['Id']\n",
        "\n",
        "train.drop('Id', axis = 1, inplace = True)\n",
        "test.drop('Id', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "al_wH_y5I1-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ntrain = train.shape[0]\n",
        "ntest = test.shape[0]\n",
        "\n",
        "y_train = train.SalePrice.values\n",
        "\n",
        "all_data = pd.concat((train,test)).reset_index(drop=True)\n",
        "all_data.drop(['SalePrice'], axis=1, inplace=True) #убираем целевой признак из общего датафрейма"
      ],
      "metadata": {
        "id": "HbxxIO3ZJTz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data.columns"
      ],
      "metadata": {
        "id": "lcL4mwaR8JFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратимся к файлу data_discription чтобы понять значение и выделить наиболее важные признаки (предоставлен на Kaggle)\n",
        "\n",
        "\n",
        "\n",
        "1. Выделяем наиболее влиятельные по нашему мнению:\n",
        "OverallQual - Общее качество\n",
        "YearBuilt - год постройки\n",
        "TotalBsmSF - площадь подземной части\n",
        "GrLivArea - площадь надземной части\n",
        "\n",
        "2. Попробуем проанализировать зависимость цены дама от этих признаков"
      ],
      "metadata": {
        "id": "6pJaRN7U8xSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Анализ цены\n",
        "\n"
      ],
      "metadata": {
        "id": "ilifvmD2-Rdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(y_train)"
      ],
      "metadata": {
        "id": "Za8Fm9tS9CCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для более точной оценки распределения существует две характеристики\n",
        "\n",
        "Skewness - коэффициент асимметрии (Насколько пик распределения отклоняется от центра)\n",
        "\n",
        "Kurtosis - коэффициент эксцесса (Острота пика распределения)"
      ],
      "metadata": {
        "id": "FPlUMUl5_e-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Skewness: {}'.format(train['SalePrice'].skew()))\n",
        "print('Kurtosis: {}'.format(train['SalePrice'].kurt()))"
      ],
      "metadata": {
        "id": "fWVyNpruAKyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Определим зависимость цены от других признаков\n",
        "\n",
        "Теперь когда мы разобрадись с анализом одной величины, попробуем посмотреть как она взаимодействует с другими величинами\n",
        "\n",
        "Ранее мы выделили 4 признака, которые кажутся потенциально значимыми.\n",
        "\n",
        "Два из них числовые(GrLivArea и TotalBsmtSF), другие два категориальные (OverallQual и YearBuilt)."
      ],
      "metadata": {
        "id": "zbfQKgd6BN_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Числовые признаки\n",
        "\n",
        "Начнем с числовых признаков. Хорошим способом посмотреть на зависимость двух признаков друг от друга является точечная диаграмма."
      ],
      "metadata": {
        "id": "e-Rf8h-nCfsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize = (15, 5))\n",
        "\n",
        "ax[0].scatter(train['GrLivArea'], train['SalePrice'])\n",
        "ax[0].set_xlabel('GrLivArea')\n",
        "ax[0].set_ylabel('SalePrice')\n",
        "\n",
        "ax[1].scatter(train['TotalBsmtSF'], train['SalePrice'])\n",
        "ax[1].set_xlabel('TotalBsmtSF')\n",
        "ax[1].set_ylabel('SalePrice')"
      ],
      "metadata": {
        "id": "tAyiTexNC4Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из диаграмм становится видно, что цена действительно сильно зависит от этих двух признаков\n",
        "\n",
        "Линейно в случае с GrLivArea и линейно или возможно экспоненциально в случае TotalBsmtSF "
      ],
      "metadata": {
        "id": "08JK_A1IEoJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Категориальные признаки\n",
        "\n",
        "Точечные диаграммы для категориальных признаков менее информативны, разуменее для их анализа будет использовать ящик с усами"
      ],
      "metadata": {
        "id": "irj8z3ZsFX11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строим для каждой категории качества(1-10)"
      ],
      "metadata": {
        "id": "ueY_onvOHUS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\n",
        "f, ax = plt.subplots(figsize=(10, 7))\n",
        "fig = sns.boxplot(x='OverallQual', y = 'SalePrice', data=data)"
      ],
      "metadata": {
        "id": "Ox5BmcgKFWmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "На графике очень хорошо видан экспоненциальная зависимость как по медиана, так и по границам квантилей. На основе этого можно предположить, что качество потенциально значимый фактор."
      ],
      "metadata": {
        "id": "zWaxgkw2HHkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично проанализируем год постройки дома"
      ],
      "metadata": {
        "id": "Ir9By6i1IEI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)\n",
        "f, ax = plt.subplots(figsize=(30, 8))\n",
        "fig = sns.boxplot(x='YearBuilt', y = 'SalePrice', data=data)"
      ],
      "metadata": {
        "id": "4lfzAGUfIJX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C годом постройки не видно хорошо описываемой зависимости, однако новые дома все же имеют тенденцию стоить дороже старых."
      ],
      "metadata": {
        "id": "0HG9V8duIrn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Общий анализ (систематический)"
      ],
      "metadata": {
        "id": "lDX6Tq0KJK8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь давайте отступим от анализа признаков, которые показались нам значимыми, и попробуем более общий подход."
      ],
      "metadata": {
        "id": "NElFN7nlJWoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Матрица корреляции"
      ],
      "metadata": {
        "id": "-gxTsbDmJsvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Хорошим инструментом для анализа зависимостей между всеми признаками является матрица корреляции"
      ],
      "metadata": {
        "id": "_btM-RrrJ0OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corrmat = train.corr()\n",
        "f, ax = plt.subplots(figsize =(12,9))\n",
        "sns.heatmap(corrmat, vmax = .8, square =True)"
      ],
      "metadata": {
        "id": "ibvP9hUgJUOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пара высококорелирующих признаков может быть заменена одним.\n",
        "\n",
        "Посмотрев на матрицу корреляции мы можем выделить наиболее значимые для предсказания признаки. Вместе с тем, мы можем найти пары признаков имеющих высокую корреляцию и исключить один из них их анализа. Что позволит выбрать наибольший набор главных признаков для дальнейшего анализа.\n",
        "\n",
        "\n",
        "Посмотрит на первые 10 признаков, имеющих наибольшую корреляцию с ценой дома"
      ],
      "metadata": {
        "id": "9ZOaK2iOMbOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corrmat.sort_values(by=['SalePrice'], ascending=False)[['SalePrice']][:10]"
      ],
      "metadata": {
        "id": "KeSwV0l-NkhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Признаки OverallQual и GrLivArea нам знакомы и мы убедились в из значимости\n",
        "\n",
        "Признаки GarageCars и GarageArea очень близки по смыслу и меют высокую корреляцию между друг другом , поэтому мы можем оставить только один из них для дальнейшего поиска зависимостей.\n",
        "\n",
        "аналогичная ситуация обстоит с парами признаков GrLivArea и TotRmsAbvGrd, TotalBsmtSF и 1stFLrSF\n",
        "\n",
        "В итоге для дальнейшего анализа мы можем остаивть только 6 главных признаков."
      ],
      "metadata": {
        "id": "4TPMJRfOOYFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Парные зависимости между признаками\n",
        "\n",
        "Теперь, когда множество признаков сильно уменьшилось, мы можем посмотреть на их зависимости между друг другом"
      ],
      "metadata": {
        "id": "xOrJIKkzKsWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set()\n",
        "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
        "sns.pairplot(train[cols], size = 2.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2GSC8oy7PvLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "На этом этапе можно совершить множество значимых наблюдений, но мы не будем затягивать анализ и сделаем только два.\n",
        "-зависимости между признаками похожи на линейные или мягкие экспоненциальные.\n",
        "- среди зависимостей можно увидеть множество \"конусов\", такие зависимости плохо описываются линейными, поэтому от них стоит избавиться с помощью специальных преобразований (Об этом позже)"
      ],
      "metadata": {
        "id": "EEP7_SK6NCKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Пропуски\n",
        "Попробуем избавиться от пропусков более осозданным методом, чем выбрасываение всех строк с пропусками. \n",
        "\n",
        "Начать стоит с того, чтобы оценить, какой процент пропусков содержится в данных по каждому признаку."
      ],
      "metadata": {
        "id": "JoiZdnyqO7sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = train.isnull().sum().sort_values(ascending=False)\n",
        "percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "missing_data.head(30)"
      ],
      "metadata": {
        "id": "Cm0vHQSpOFDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total1 = all_data.isnull().sum().sort_values(ascending=False)\n",
        "percent = (all_data.isnull().sum()/all_data.isnull().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total1, percent], axis=1, keys=['Total', 'Percent'])\n",
        "missing_data.head(40)"
      ],
      "metadata": {
        "id": "_HSXEpNqLfSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разумно будет отсечь часть признаков, имеющих очень высокий процент пропусков и не пытаться их заполнить. Выкинем все признаки имеющие процент пропуска больше 0.15"
      ],
      "metadata": {
        "id": "FNG2bdmGRZDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop((missing_data[missing_data['Percent']>0.15]).index,1)"
      ],
      "metadata": {
        "id": "djLHBPZ5jxXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = all_data.drop((missing_data[missing_data['Percent']>0.15]).index,1)"
      ],
      "metadata": {
        "id": "gJgZfcQxRUA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Остальные пропуски заполним соответствующими логике признака значениями 'пропуска'(0, 'None', etc)."
      ],
      "metadata": {
        "id": "sfcaaSPSSY90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
        "  train[col] = train[col].fillna('None')\n",
        "  all_data[col] = all_data[col].fillna('None')"
      ],
      "metadata": {
        "id": "JiXkSumVS9n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
        "  train[col] = train[col].fillna(0)\n",
        "  all_data[col] = all_data[col].fillna(0)"
      ],
      "metadata": {
        "id": "3ypCeup9Tm0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
        "  train[col] = train[col].fillna(0)\n",
        "  all_data[col] = all_data[col].fillna(0)"
      ],
      "metadata": {
        "id": "rwMUOb2FT5kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
        "  train[col] = train[col].fillna('None')\n",
        "  all_data[col] = all_data[col].fillna('None')"
      ],
      "metadata": {
        "id": "pS9CnwnMVDs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['MasVnrType'] = train['MasVnrType'].fillna('None')\n",
        "all_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\n",
        "\n",
        "train['MasVnrArea'] = train['MasVnrArea'].fillna(0)\n",
        "all_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)"
      ],
      "metadata": {
        "id": "D0JDlUpxVq0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['Electrical'] = train['Electrical'].fillna(train ['Electrical'].mode()[0]) #заполняем модой\n",
        "all_data['Electrical'] = all_data['Electrical'].fillna(all_data ['Electrical'].mode()[0]) #заполняем модой\n",
        "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data ['MSZoning'].mode()[0]) #заполняем модой\n",
        "all_data['Utilities'] = all_data['Utilities'].fillna(all_data ['Utilities'].mode()[0]) #заполняем модой\n",
        "all_data['Functional'] = all_data['Functional'].fillna(all_data ['Functional'].mode()[0]) #заполняем модой\n",
        "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data ['KitchenQual'].mode()[0]) #заполняем модой\n",
        "all_data['SaleType'] = all_data['SaleType'].fillna(all_data ['SaleType'].mode()[0]) #заполняем модой\n",
        "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data ['Exterior2nd'].mode()[0]) #заполняем модой\n",
        "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data ['Exterior1st'].mode()[0]) #заполняем модой\n"
      ],
      "metadata": {
        "id": "z4qrkYSOWRvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Нормальное распределение\n",
        "\n",
        "Многие методы машинного обучения опираются на нормальное распределение. И в общем случае, желательно работать с данными распределение которых_нормально.\n",
        "\n",
        "Посмотрим как распределены наши признаки"
      ],
      "metadata": {
        "id": "HbK0AW5NXgM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(train['SalePrice'], fit=norm);\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(train['SalePrice'], plot=plt)"
      ],
      "metadata": {
        "id": "7tXbGgBMXfs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "хорошо заметно, что распределение далеко от нормального и имеет высокий положительный коэффициент ассиметрии. В таком случае к распределению можно применить функцию логарифма и 'выпрямить\" распределение."
      ],
      "metadata": {
        "id": "Vjdu1sJeadct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.log(train['SalePrice'])"
      ],
      "metadata": {
        "id": "wM27ccaIa7du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(y_train, fit=norm)\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(y_train, plot=plt)"
      ],
      "metadata": {
        "id": "JAv4sFOTbJoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "На точечных диаграммах такие зависимости похоже на конусы, о которых упоминалось ранее.\n",
        "\n",
        "Однако нам удается избавиться от них.\n",
        "\n",
        "Применим этот подод к другим признакам имеющим положительный коэффициент ассиметрии"
      ],
      "metadata": {
        "id": "J8lg3zOSb9wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['GrLivArea'] = np.log(train['GrLivArea'])\n",
        "all_data['GrLivArea'] = np.log(all_data['GrLivArea'])\n",
        "train.loc[train['TotalBsmtSF'] > 0, 'TotalBsmtSF']=np.log(train['TotalBsmtSF'])#Пропустим случаи с 0 значениями\n",
        "all_data.loc[all_data['TotalBsmtSF'] > 0, 'TotalBsmtSF']=np.log(all_data['TotalBsmtSF'])#Пропустим случаи с 0 значениями\n"
      ],
      "metadata": {
        "id": "YrdJNHcacc-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если теперь посмотреть на точечные диаграммы зависимостей, то можно увидеть, что конусов больше нет."
      ],
      "metadata": {
        "id": "aybGrIZmd8zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set()\n",
        "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
        "sns.pairplot(train[cols], size = 2.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NMLEbb4zeMdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Категориальные признаки (кодирование)\n",
        "\n",
        "Среди признаков есть те, что представлены числами, но на самом деле являются категориальными. Просто преобразуем их в строки."
      ],
      "metadata": {
        "id": "MzaP6PIVhEoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['MSSubClass']=train['MSSubClass'].apply(str)\n",
        "train['OverallCond']=train['OverallCond'].astype(str)\n",
        "train['YrSold']=train['YrSold'].astype(str)\n",
        "train['MoSold']=train['MoSold'].astype(str)\n",
        "all_data['MSSubClass']=all_data['MSSubClass'].apply(str)\n",
        "all_data['OverallCond']=all_data['OverallCond'].astype(str)\n",
        "all_data['YrSold']=all_data['YrSold'].astype(str)\n",
        "all_data['MoSold']=all_data['MoSold'].astype(str)"
      ],
      "metadata": {
        "id": "36lgYuuahfBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Также есть категориальные признаки, которые принципиальнобудут иметь больше смысла , если закодировать их с помощью метода label encoding\n",
        "\n",
        "В случае с one-hot encoding теряется упорядоченность категорий, label encoding эту упорядоченность сохраняет"
      ],
      "metadata": {
        "id": "4g5rfSAOijsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ( 'ExterQual', 'ExterCond', 'HeatingQC',  'KitchenQual',\n",
        "        'Functional', 'LandSlope', 'LotShape',\n",
        "       'PavedDrive', 'CentralAir', 'MSSubClass', 'OverallCond', 'MoSold')\n",
        "\n",
        "for c in cols:\n",
        "   lbl1 = LabelEncoder()\n",
        "   lbl1.fit(list(all_data[c].values))\n",
        "   all_data[c] = lbl1.transform(list(all_data[c].values))"
      ],
      "metadata": {
        "id": "2SHAWLGeZA-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оставшиеся категориальные признаки закодируем методом one-hot encoder"
      ],
      "metadata": {
        "id": "n2hE6ZzxmJiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.get_dummies(all_data)"
      ],
      "metadata": {
        "id": "p1bK3RWiZdSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = all_data[:ntrain]\n",
        "test = all_data[ntrain:]"
      ],
      "metadata": {
        "id": "wN_AjPS-Nm_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Модель"
      ],
      "metadata": {
        "id": "SCkaq-p-o0fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.special"
      ],
      "metadata": {
        "id": "sd2du91GVVm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "4JdiTMYSo4Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Стекинг\n",
        "\n",
        "На сегодняшний день существует множество методов решения регрессионных задач. Однако вместо того, чтобы выбирать один метод, мы выберем 4 сразу.\n",
        "\n",
        "Не редко в машинном обучении используются ансамбли моделей, несколько моделей, предсказание которых усредняется.\n",
        "\n",
        "Методов объединения моделей в ансамбли несколько, но в этот раз мы используем один - Стекинг.\n",
        "\n",
        "За стекингом стоит очень простая идея. Каждая модель в ансамбле обучается отдельно, а после создается мета-модель, которая только на основе предсказаний моделей ансамбля предсказывает единое усредненное предсказание."
      ],
      "metadata": {
        "id": "JmQBo0sVtPYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Кросс-валидация\n",
        "\n",
        "Перед переходом к стекингу и ансамблям изменим наш способ оценки работы модели\n",
        "\n",
        "Обучать модель полностью на тренировочной выборке и тестировать ее на тренировочной выборке - это плохой подход. Если модель переобучится, то мы это никак не заметим.\n",
        "\n",
        "Разделим тренировочнуб выборку данных на тренировочную и валидационную - более хороший подход.\n",
        "\n",
        "Но наиболее хорошим вариантом будет использовать кросс-валидацию.\n",
        "\n",
        "1. Данные делятся на несколько равных частей.\n",
        "2. Первая часть выбирется валидационной.\n",
        "3. Модель обучается на всех данных, кроме валидационной части.\n",
        "4. Первые три шага повторяются, но на втором берется не первая часть, а вторая , потом третья и.т.д\n",
        "5. Результаты оценки усредняются"
      ],
      "metadata": {
        "id": "cSmJx7GwvBc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "n_folds = 5\n",
        "\n",
        "def rmse_cv(model, X, y):\n",
        "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n",
        "    rmse = np.sqrt(-cross_val_score(model, X.values, y.values, scoring='neg_mean_squared_error', cv=kf))\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "oG4jWoCab8kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Регрессионные модели"
      ],
      "metadata": {
        "id": "VbXIl-4wyn9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объявим четыре модели и оценим их точность\n",
        "\n",
        "Отдельно стоит изучить выбор модели и подбор гипер-параметров"
      ],
      "metadata": {
        "id": "0FujdyE-y3ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0004, random_state=1))"
      ],
      "metadata": {
        "id": "JpMg-a_rwu9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0004, l1_ratio=.9, random_state=3))"
      ],
      "metadata": {
        "id": "H2fc1z44zmEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KRR = KernelRidge(alpha=0.5, kernel='polynomial', gamma=0.2, degree=1, coef0=3)"
      ],
      "metadata": {
        "id": "FH4YbwvC0EdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GBoost = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=5)"
      ],
      "metadata": {
        "id": "7cewy7ES0i2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модели lasso и ENet чувствительны в выбросам значений, поэтому данные сначала нормализуются с помощью RobustScaler"
      ],
      "metadata": {
        "id": "wKddfAkJ2DKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценим их точность по отдельности"
      ],
      "metadata": {
        "id": "ShX0QtrY3pl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_reset = train.reset_index(drop=True)\n",
        "target_reset = y_train.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "DxyzQvEYa5Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = rmse_cv(lasso,train,y_train).mean()\n",
        "print('Lasso score: {}'.format(round(score, 5)))\n",
        "score = rmse_cv(ENet,train, y_train).mean()\n",
        "print('Enet score: {}'.format(round(score, 5)))\n",
        "score = rmse_cv(KRR,train,y_train).mean()\n",
        "print('KRR score: {}'.format(round(score, 5)))\n",
        "score = rmse_cv(GBoost,train, y_train).mean()\n",
        "print('Gboost score: {}'.format(round(score, 5)))"
      ],
      "metadata": {
        "id": "nwkBRrBA3jly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Видим что впереди находится GBoost"
      ],
      "metadata": {
        "id": "eHvSwHiA6FGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ансамбль(усреднение)"
      ],
      "metadata": {
        "id": "2mFH1Kk6mbiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь объединяем модели в ансамбли\n",
        "\n",
        "Перед тем, как перейти к использованию мета-модели, попробуем просто усреднить их предсказания."
      ],
      "metadata": {
        "id": "4MsSIrd46j9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.models_ = [clone(x) for x in self.models]\n",
        "\n",
        "        # Обучаем модель\n",
        "        for model in self.models_:\n",
        "            model.fit(X, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Находим предсказание для каждой модели\n",
        "        predictions = np.column_stack([\n",
        "            model.predict(X) for model in self.models_\n",
        "        ])\n",
        "\n",
        "        # Усредняем предсказание\n",
        "        return np.mean(predictions, axis=1)"
      ],
      "metadata": {
        "id": "7TVRSjyiRj5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим какую точность имеет такой ансамбль"
      ],
      "metadata": {
        "id": "p7bDlpNPEcEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "averaged_models = AveragingModels(models = (ENet, lasso, KRR, GBoost))\n",
        "# сбрасываем индексы для train и target\n",
        "train_reset = train.reset_index(drop=True)\n",
        "target_reset = y_train.reset_index(drop=True)\n",
        "\n",
        "# запускаем кросс-валидацию с новыми индексами\n",
        "score = rmse_cv(averaged_models, train_reset, target_reset).mean()\n",
        "print('Stacking score: {}'.format(round(score,5)))"
      ],
      "metadata": {
        "id": "k647cWYphcOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "averaged_models.fit(train.values, y_train)"
      ],
      "metadata": {
        "id": "jKtxopiucL1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "averaged_pred = averaged_models.predict(test)"
      ],
      "metadata": {
        "id": "V0gki6gZcahO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salePrice_pred = np.exp(averaged_pred)"
      ],
      "metadata": {
        "id": "tm4uz5PRcxfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Сабмит №1"
      ],
      "metadata": {
        "id": "eDSXwzFTlq1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# creating submission directory if it does not exist\n",
        "if not os.path.exists('submission'):\n",
        "    os.makedirs('submission')\n",
        "\n",
        "sub = pd.DataFrame()\n",
        "sub['Id']=test_ID\n",
        "sub['SalePrice'] = salePrice_pred\n",
        "sub.to_csv('submission/basic_submission4.csv', index=False)"
      ],
      "metadata": {
        "id": "nuYF4ggSxgYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "При простом усреднении нам удалось добиться большей точности, чем лучшая модель давала по отдельности\n",
        "\n",
        "Заменим усреднение на мета-модель"
      ],
      "metadata": {
        "id": "NBOUFP_6F12m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ансамбль с метамоделью\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "W8bcsG8ZOwl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "  def __init__(self, base_models, meta_model, n_folds=5):\n",
        "    self.base_models = base_models\n",
        "    self.meta_model = meta_model\n",
        "    self.n_folds = n_folds\n",
        "    self.base_models_ = [list() for _ in self.base_models]\n",
        "\n",
        "  def fit(self, X, y):\n",
        "      self.base_models_ = [list() for x in self.base_models]\n",
        "      self.meta_model_ = clone(self.meta_model)\n",
        "      kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
        "      out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
        "      for i, model in enumerate(self.base_models):\n",
        "        for train_index, holdout_index in kfold.split(X, y):\n",
        "          instance = clone(model)\n",
        "          self.base_models_[i].append(instance)\n",
        "          instance.fit(X[train_index], y[train_index])\n",
        "          y_pred = instance.predict(X[holdout_index])\n",
        "          out_of_fold_predictions[holdout_index, i] = y_pred\n",
        "\n",
        "      self.meta_model_.fit(out_of_fold_predictions, y)\n",
        "      return self\n",
        "\n",
        "  def predict(self, X):\n",
        "    meta_features = np.column_stack([\n",
        "        np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
        "        for base_models in self.base_models_])\n",
        "    return self.meta_model_.predict(meta_features)"
      ],
      "metadata": {
        "id": "zhyWu1YHTxL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на точность такого ансамбля (в качестве мета-модели используется Lasso)"
      ],
      "metadata": {
        "id": "vZWphsrxN5P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_averaged_models = StackingAveragedModels(base_models=(ENet, GBoost, KRR,lasso),\n",
        "                                                 meta_model=lasso)\n",
        "# сбрасываем индексы для train и target\n",
        "train_reset = train.reset_index(drop=True)\n",
        "target_reset = y_train.reset_index(drop=True)\n",
        "\n",
        "# запускаем кросс-валидацию с новыми индексами\n",
        "score = rmse_cv(stacked_averaged_models, train_reset, target_reset).mean()\n",
        "print('Stacking score: {}'.format(round(score,5)))\n"
      ],
      "metadata": {
        "id": "oYhr68t4CDqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_averaged_models.fit(train.values, y_train)"
      ],
      "metadata": {
        "id": "U4rqP9aqdnj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_pred = stacked_averaged_models.predict(test)"
      ],
      "metadata": {
        "id": "u0pPQPSCdwio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salePrice_pred = np.exp(stacked_pred)"
      ],
      "metadata": {
        "id": "8YW5CAFimCVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сабмит№2"
      ],
      "metadata": {
        "id": "3AiOZwttmrNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# creating submission directory if it does not exist\n",
        "if not os.path.exists('submission'):\n",
        "    os.makedirs('submission')\n",
        "\n",
        "sub = pd.DataFrame()\n",
        "sub['Id']=test_ID\n",
        "sub['SalePrice'] = salePrice_pred\n",
        "sub.to_csv('submission/basic_submission_8.csv', index=False)"
      ],
      "metadata": {
        "id": "I0sKMKS8ekV3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}